{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas importantes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\018118631\\AppData\\Local\\Temp\\ipykernel_66324\\2754489263.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews1_df = pd.read_csv(\"reviews_0-250.csv\")\n",
      "C:\\Users\\018118631\\AppData\\Local\\Temp\\ipykernel_66324\\2754489263.py:5: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews4_df = pd.read_csv(\"reviews_750-1250.csv\")\n",
      "C:\\Users\\018118631\\AppData\\Local\\Temp\\ipykernel_66324\\2754489263.py:6: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews5_df = pd.read_csv(\"reviews_1250-end.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Importando os csvs\n",
    "reviews1_df = pd.read_csv(\"reviews_0-250.csv\")\n",
    "reviews2_df = pd.read_csv(\"reviews_250-500.csv\")\n",
    "reviews3_df = pd.read_csv(\"reviews_500-750.csv\")\n",
    "reviews4_df = pd.read_csv(\"reviews_750-1250.csv\")\n",
    "reviews5_df = pd.read_csv(\"reviews_1250-end.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_reviews shape:  (1094411, 19)\n"
     ]
    }
   ],
   "source": [
    "# Juntando os dataframes de reviews\n",
    "df_reviews = pd.concat([reviews1_df,reviews2_df,reviews3_df,reviews4_df,reviews5_df],axis=0)\n",
    "print(\"df_reviews shape: \",df_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>is_recommended</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>total_feedback_count</th>\n",
       "      <th>total_neg_feedback_count</th>\n",
       "      <th>total_pos_feedback_count</th>\n",
       "      <th>submission_time</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_title</th>\n",
       "      <th>skin_tone</th>\n",
       "      <th>eye_color</th>\n",
       "      <th>skin_type</th>\n",
       "      <th>hair_color</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1741593524</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>I use this with the Nudestix “Citrus Clean Bal...</td>\n",
       "      <td>Taught me how to double cleanse!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>brown</td>\n",
       "      <td>dry</td>\n",
       "      <td>black</td>\n",
       "      <td>P504322</td>\n",
       "      <td>Gentle Hydra-Gel Face Cleanser</td>\n",
       "      <td>NUDESTIX</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31423088263</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>I bought this lip mask after reading the revie...</td>\n",
       "      <td>Disappointed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P420652</td>\n",
       "      <td>Lip Sleeping Mask Intense Hydration with Vitam...</td>\n",
       "      <td>LANEIGE</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5061282401</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>My review title says it all! I get so excited ...</td>\n",
       "      <td>New Favorite Routine</td>\n",
       "      <td>light</td>\n",
       "      <td>brown</td>\n",
       "      <td>dry</td>\n",
       "      <td>blonde</td>\n",
       "      <td>P420652</td>\n",
       "      <td>Lip Sleeping Mask Intense Hydration with Vitam...</td>\n",
       "      <td>LANEIGE</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6083038851</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>I’ve always loved this formula for a long time...</td>\n",
       "      <td>Can't go wrong with any of them</td>\n",
       "      <td>NaN</td>\n",
       "      <td>brown</td>\n",
       "      <td>combination</td>\n",
       "      <td>black</td>\n",
       "      <td>P420652</td>\n",
       "      <td>Lip Sleeping Mask Intense Hydration with Vitam...</td>\n",
       "      <td>LANEIGE</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>47056667835</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>If you have dry cracked lips, this is a must h...</td>\n",
       "      <td>A must have !!!</td>\n",
       "      <td>light</td>\n",
       "      <td>hazel</td>\n",
       "      <td>combination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P420652</td>\n",
       "      <td>Lip Sleeping Mask Intense Hydration with Vitam...</td>\n",
       "      <td>LANEIGE</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    author_id  rating  is_recommended  helpfulness  \\\n",
       "0           0   1741593524       5             1.0          1.0   \n",
       "1           1  31423088263       1             0.0          NaN   \n",
       "2           2   5061282401       5             1.0          NaN   \n",
       "3           3   6083038851       5             1.0          NaN   \n",
       "4           4  47056667835       5             1.0          NaN   \n",
       "\n",
       "   total_feedback_count  total_neg_feedback_count  total_pos_feedback_count  \\\n",
       "0                     2                         0                         2   \n",
       "1                     0                         0                         0   \n",
       "2                     0                         0                         0   \n",
       "3                     0                         0                         0   \n",
       "4                     0                         0                         0   \n",
       "\n",
       "  submission_time                                        review_text  \\\n",
       "0      2023-02-01  I use this with the Nudestix “Citrus Clean Bal...   \n",
       "1      2023-03-21  I bought this lip mask after reading the revie...   \n",
       "2      2023-03-21  My review title says it all! I get so excited ...   \n",
       "3      2023-03-20  I’ve always loved this formula for a long time...   \n",
       "4      2023-03-20  If you have dry cracked lips, this is a must h...   \n",
       "\n",
       "                       review_title skin_tone eye_color    skin_type  \\\n",
       "0  Taught me how to double cleanse!       NaN     brown          dry   \n",
       "1                      Disappointed       NaN       NaN          NaN   \n",
       "2              New Favorite Routine     light     brown          dry   \n",
       "3   Can't go wrong with any of them       NaN     brown  combination   \n",
       "4                   A must have !!!     light     hazel  combination   \n",
       "\n",
       "  hair_color product_id                                       product_name  \\\n",
       "0      black    P504322                     Gentle Hydra-Gel Face Cleanser   \n",
       "1        NaN    P420652  Lip Sleeping Mask Intense Hydration with Vitam...   \n",
       "2     blonde    P420652  Lip Sleeping Mask Intense Hydration with Vitam...   \n",
       "3      black    P420652  Lip Sleeping Mask Intense Hydration with Vitam...   \n",
       "4        NaN    P420652  Lip Sleeping Mask Intense Hydration with Vitam...   \n",
       "\n",
       "  brand_name  price_usd  \n",
       "0   NUDESTIX       19.0  \n",
       "1    LANEIGE       24.0  \n",
       "2    LANEIGE       24.0  \n",
       "3    LANEIGE       24.0  \n",
       "4    LANEIGE       24.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                    int64\n",
      "author_id                    object\n",
      "rating                        int64\n",
      "is_recommended              float64\n",
      "helpfulness                 float64\n",
      "total_feedback_count          int64\n",
      "total_neg_feedback_count      int64\n",
      "total_pos_feedback_count      int64\n",
      "submission_time              object\n",
      "review_text                  object\n",
      "review_title                 object\n",
      "skin_tone                    object\n",
      "eye_color                    object\n",
      "skin_type                    object\n",
      "hair_color                   object\n",
      "product_id                   object\n",
      "product_name                 object\n",
      "brand_name                   object\n",
      "price_usd                   float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_reviews.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                       0\n",
       "author_id                        0\n",
       "rating                           0\n",
       "is_recommended              167988\n",
       "helpfulness                 561592\n",
       "total_feedback_count             0\n",
       "total_neg_feedback_count         0\n",
       "total_pos_feedback_count         0\n",
       "submission_time                  0\n",
       "review_text                   1444\n",
       "review_title                310654\n",
       "skin_tone                   170539\n",
       "eye_color                   209628\n",
       "skin_type                   111557\n",
       "hair_color                  226768\n",
       "product_id                       0\n",
       "product_name                     0\n",
       "brand_name                       0\n",
       "price_usd                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando a quantidade de valores nulos\n",
    "df_reviews.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropar a coluna unnamed, que é causada por um erro nos csvs\n",
    "df_reviews.drop(columns=\"Unnamed: 0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author_id', 'rating', 'is_recommended', 'helpfulness',\n",
       "       'total_feedback_count', 'total_neg_feedback_count',\n",
       "       'total_pos_feedback_count', 'submission_time', 'review_text',\n",
       "       'review_title', 'skin_tone', 'eye_color', 'skin_type', 'hair_color',\n",
       "       'product_id', 'product_name', 'brand_name', 'price_usd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando as colunas\n",
    "df_reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0., nan])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando como são os dados da coluna \"is_recommended\"\n",
    "df_reviews[\"is_recommended\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>is_recommended</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>total_feedback_count</th>\n",
       "      <th>total_neg_feedback_count</th>\n",
       "      <th>total_pos_feedback_count</th>\n",
       "      <th>price_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.094411e+06</td>\n",
       "      <td>926423.000000</td>\n",
       "      <td>532819.000000</td>\n",
       "      <td>1.094411e+06</td>\n",
       "      <td>1.094411e+06</td>\n",
       "      <td>1.094411e+06</td>\n",
       "      <td>1.094411e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.299158e+00</td>\n",
       "      <td>0.839962</td>\n",
       "      <td>0.767782</td>\n",
       "      <td>4.177126e+00</td>\n",
       "      <td>8.948695e-01</td>\n",
       "      <td>3.282257e+00</td>\n",
       "      <td>4.900838e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.149444e+00</td>\n",
       "      <td>0.366642</td>\n",
       "      <td>0.317164</td>\n",
       "      <td>2.271524e+01</td>\n",
       "      <td>5.288943e+00</td>\n",
       "      <td>1.967482e+01</td>\n",
       "      <td>4.004338e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.500000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.900000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>6.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.464000e+03</td>\n",
       "      <td>1.159000e+03</td>\n",
       "      <td>5.050000e+03</td>\n",
       "      <td>1.900000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rating  is_recommended    helpfulness  total_feedback_count  \\\n",
       "count  1.094411e+06   926423.000000  532819.000000          1.094411e+06   \n",
       "mean   4.299158e+00        0.839962       0.767782          4.177126e+00   \n",
       "std    1.149444e+00        0.366642       0.317164          2.271524e+01   \n",
       "min    1.000000e+00        0.000000       0.000000          0.000000e+00   \n",
       "25%    4.000000e+00        1.000000       0.652174          0.000000e+00   \n",
       "50%    5.000000e+00        1.000000       0.928571          0.000000e+00   \n",
       "75%    5.000000e+00        1.000000       1.000000          3.000000e+00   \n",
       "max    5.000000e+00        1.000000       1.000000          5.464000e+03   \n",
       "\n",
       "       total_neg_feedback_count  total_pos_feedback_count     price_usd  \n",
       "count              1.094411e+06              1.094411e+06  1.094411e+06  \n",
       "mean               8.948695e-01              3.282257e+00  4.900838e+01  \n",
       "std                5.288943e+00              1.967482e+01  4.004338e+01  \n",
       "min                0.000000e+00              0.000000e+00  3.000000e+00  \n",
       "25%                0.000000e+00              0.000000e+00  2.500000e+01  \n",
       "50%                0.000000e+00              0.000000e+00  3.900000e+01  \n",
       "75%                1.000000e+00              3.000000e+00  6.200000e+01  \n",
       "max                1.159000e+03              5.050000e+03  1.900000e+03  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando métricas estatísticas\n",
    "df_reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se que 75% dos dados da coluna rating são maiores que 4, comportamento sustentado pela coluna is_recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verificar a distribuição da coluna \"is_recommended\"\n",
    "df_reviews[\"is_recommended\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar as colunas que não serão necessárias para o projeto\n",
    "columns_notsentiment = ['author_id', 'rating', 'helpfulness',\n",
    "       'total_feedback_count', 'total_neg_feedback_count',\n",
    "       'total_pos_feedback_count', 'submission_time',\n",
    "       'review_title', 'skin_tone', 'eye_color', 'skin_type', 'hair_color',\n",
    "       'product_id', 'product_name', 'brand_name', 'price_usd']\n",
    "\n",
    "# Dropando as colunas\n",
    "df_sentiment = df_reviews.drop(columns_notsentiment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_recommended    1094411\n",
       "review_text       1094411\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando valores nulos\n",
    "df_sentiment.isnull().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropar linhas nulas\n",
    "df_sentiment.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_recommended</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I use this with the Nudestix “Citrus Clean Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>I bought this lip mask after reading the revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>My review title says it all! I get so excited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I’ve always loved this formula for a long time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>If you have dry cracked lips, this is a must h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_recommended                                        review_text\n",
       "0             1.0  I use this with the Nudestix “Citrus Clean Bal...\n",
       "1             0.0  I bought this lip mask after reading the revie...\n",
       "2             1.0  My review title says it all! I get so excited ...\n",
       "3             1.0  I’ve always loved this formula for a long time...\n",
       "4             1.0  If you have dry cracked lips, this is a must h..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trocando os valores da coluna \"is_recommended\" para -1 e 1 e trocando o nome\n",
    "df_sentiment['label'] = df_sentiment['is_recommended'].map({0: -1, 1: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deixando o nome da coluna \"review_text\" para um mais simples\n",
    "df_sentiment['review'] = df_sentiment[\"review_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = ['is_recommended', 'review_text']\n",
    "\n",
    "df_sentiment.drop(colunas, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I use this with the Nudestix “Citrus Clean Bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>I bought this lip mask after reading the revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>My review title says it all! I get so excited ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I’ve always loved this formula for a long time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>If you have dry cracked lips, this is a must h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  I use this with the Nudestix “Citrus Clean Bal...\n",
       "1     -1  I bought this lip mask after reading the revie...\n",
       "2      1  My review title says it all! I get so excited ...\n",
       "3      1  I’ve always loved this formula for a long time...\n",
       "4      1  If you have dry cracked lips, this is a must h..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diminuindo o tamanho da amostra para diminuir o tempo de processamento\n",
    "df_amostra = df_sentiment.sample(n=3000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o VADER\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        label                                             review    neg  \\\n",
      "19619       1  I love the moisturizer! It has such a great te...  0.000   \n",
      "208964      1  So amazing it smells so good and doesnt dry my...  0.000   \n",
      "142324      1  I absolutely love this face mask. I use it eve...  0.100   \n",
      "7492        1  Gifted by the outset. This is the first time i...  0.025   \n",
      "569881      1  Love this stuff!  Makes my skin feel great wit...  0.000   \n",
      "\n",
      "          neu    pos  compound  \n",
      "19619   0.751  0.249    0.9508  \n",
      "208964  0.641  0.359    0.8331  \n",
      "142324  0.781  0.119   -0.3407  \n",
      "7492    0.762  0.213    0.9334  \n",
      "569881  0.512  0.488    0.8622  \n"
     ]
    }
   ],
   "source": [
    "# Aplicando o VADER na amostra\n",
    "for index, row in df_amostra.iterrows():\n",
    "    text = row['review']\n",
    "\n",
    "    scores = sid.polarity_scores(text)\n",
    "\n",
    "    df_amostra.at[index, 'neg'] = scores['neg']\n",
    "    df_amostra.at[index, 'neu'] = scores['neu']\n",
    "    df_amostra.at[index, 'pos'] = scores['pos']\n",
    "    df_amostra.at[index, 'compound'] = scores['compound']\n",
    "\n",
    "\n",
    "print(df_amostra.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        label                                             review    neg  \\\n",
      "98217       1  Daily must needed . As I don’t do makeup every...  0.000   \n",
      "464617      1  I’ve repurchased this twice because it hydrate...  0.000   \n",
      "146862      1  this lightweight gel-cream moisturizer. Made m...  0.000   \n",
      "174808      1  Amo este humectante, se seca rápido y la piel ...  0.000   \n",
      "188055      1  I put this on many hours ago and haven’t put a...  0.000   \n",
      "480573      1  ok, im gonna keep it 100 here, i have been usi...  0.000   \n",
      "15361       1  Expensive yet it works. I use it together with...  0.000   \n",
      "110160      1  This product helped when adding moisture that ...  0.000   \n",
      "21374      -1  Stinging/burning sensitive skin (No sensitivit...  0.000   \n",
      "185655     -1  Dint work for me.Used it for months but my acn...  0.000   \n",
      "71308       1  This is my go to nighttime serum. I really saw...  0.000   \n",
      "500543     -1  This balm does not emulsify completely and lea...  0.000   \n",
      "164874      1                              Will change your life  0.000   \n",
      "176190      1    This is a review for the Green Tea - Mattifying  0.000   \n",
      "207269      1  Only toner I’ve found that doesn’t break me ou...  0.000   \n",
      "54582       1  this product gave me an instant glow and made ...  0.000   \n",
      "52744       1  Only thing that has ever actually worked to re...  0.000   \n",
      "20654       1  I normally wear water proof makeup for everyth...  0.000   \n",
      "145459     -1  Used it as a mask and after I wash it off my s...  0.000   \n",
      "206244     -1  This product did literally nothing for my skin...  0.000   \n",
      "78242       1  It’s a decent cleanser however my skin felt dr...  0.000   \n",
      "49062       1  Oh a mamá mía, this is-a some-a molto spicy sa...  0.000   \n",
      "427246      1  Cleans everything off my face and makes it as ...  0.000   \n",
      "26129       1  Ummm... not to be dramatic or anything but... ...  0.000   \n",
      "51011       1  I have sensitive skin and this does not give m...  0.000   \n",
      "577129      1  Does exactly what is says it does. Effortlessl...  0.000   \n",
      "467622      1  I use this nightly as an evening eye treatment...  0.000   \n",
      "369580     -1           Overpriced and doesn’t actually hydrate.  0.000   \n",
      "450172      1  I have use this for YEARS. as soon as I notice...  0.000   \n",
      "298581      1  This is part of my daily skincare routine. I c...  0.000   \n",
      "111673     -1  It’s water. This has the same effect as sprayi...  0.000   \n",
      "54105       1  I live in Buffalo, and with dry to normal skin...  0.000   \n",
      "3446       -1  Used only twice...Allergic Reaction! Way too w...  0.000   \n",
      "323386      1  This cream will instantly fix your dry winter ...  0.000   \n",
      "72178      -1          I have dry skin. This didn’t work for me.  0.000   \n",
      "381208      1  Reduces my hereditary bags, literally deflated...  0.000   \n",
      "463838      1  Feels refreshing when I use it. I have very oi...  0.000   \n",
      "70671      -1  Pads are somewhat dry. The jar wasn’t opened w...  0.000   \n",
      "438012      1  J’ai reçu ce produit gratuitement de la part d...  0.000   \n",
      "326104      1  I could never go back to a regular run of the ...  0.000   \n",
      "86509      -1  I tried this product and o feel like it didn’t...  0.048   \n",
      "473609      1  J’adorée ca rendue ma peau très belle et lisse...  0.000   \n",
      "115050     -1  too dark for light color skin. my foundation i...  0.000   \n",
      "384069      1  Really smoothes and moisturizes My 60 year old...  0.000   \n",
      "114799      1  Smaller then I thought, but I only used this o...  0.000   \n",
      "179268      1  It really helped with my dehydrated skin quick...  0.000   \n",
      "570502      1  I have only been using this product for a coup...  0.000   \n",
      "390587     -1  I typically have oilier skin, but every time I...  0.000   \n",
      "8051       -1  Makes my sensitive skin burn. May not do this ...  0.000   \n",
      "99746      -1  This product is NOT made for Black people or r...  0.000   \n",
      "380859      1            Has helped so much I use it religiously  0.000   \n",
      "233215      1  I received a sample size of this and it has qu...  0.000   \n",
      "28323       1  Ce produit est parfait si vous avez de l’acné ...  0.000   \n",
      "41166       1  Have been using this product for about 2.5 wee...  0.000   \n",
      "38455       1  From the first days of using it I started to f...  0.000   \n",
      "175939     -1  Ordinary. Much prefer Nuface double sided clot...  0.000   \n",
      "68306       1  helped heal my dryass skin, makes ur skin soft...  0.000   \n",
      "168025     -1                     too oily and my skin break out  0.000   \n",
      "\n",
      "          neu    pos  compound  \n",
      "98217   1.000  0.000       0.0  \n",
      "464617  1.000  0.000       0.0  \n",
      "146862  1.000  0.000       0.0  \n",
      "174808  1.000  0.000       0.0  \n",
      "188055  1.000  0.000       0.0  \n",
      "480573  1.000  0.000       0.0  \n",
      "15361   1.000  0.000       0.0  \n",
      "110160  1.000  0.000       0.0  \n",
      "21374   1.000  0.000       0.0  \n",
      "185655  1.000  0.000       0.0  \n",
      "71308   1.000  0.000       0.0  \n",
      "500543  1.000  0.000       0.0  \n",
      "164874  1.000  0.000       0.0  \n",
      "176190  1.000  0.000       0.0  \n",
      "207269  1.000  0.000       0.0  \n",
      "54582   1.000  0.000       0.0  \n",
      "52744   1.000  0.000       0.0  \n",
      "20654   1.000  0.000       0.0  \n",
      "145459  1.000  0.000       0.0  \n",
      "206244  1.000  0.000       0.0  \n",
      "78242   1.000  0.000       0.0  \n",
      "49062   1.000  0.000       0.0  \n",
      "427246  1.000  0.000       0.0  \n",
      "26129   1.000  0.000       0.0  \n",
      "51011   1.000  0.000       0.0  \n",
      "577129  1.000  0.000       0.0  \n",
      "467622  1.000  0.000       0.0  \n",
      "369580  1.000  0.000       0.0  \n",
      "450172  1.000  0.000       0.0  \n",
      "298581  1.000  0.000       0.0  \n",
      "111673  1.000  0.000       0.0  \n",
      "54105   1.000  0.000       0.0  \n",
      "3446    1.000  0.000       0.0  \n",
      "323386  1.000  0.000       0.0  \n",
      "72178   1.000  0.000       0.0  \n",
      "381208  1.000  0.000       0.0  \n",
      "463838  1.000  0.000       0.0  \n",
      "70671   1.000  0.000       0.0  \n",
      "438012  1.000  0.000       0.0  \n",
      "326104  1.000  0.000       0.0  \n",
      "86509   0.904  0.048      -0.0  \n",
      "473609  1.000  0.000       0.0  \n",
      "115050  1.000  0.000       0.0  \n",
      "384069  1.000  0.000       0.0  \n",
      "114799  1.000  0.000       0.0  \n",
      "179268  1.000  0.000       0.0  \n",
      "570502  1.000  0.000       0.0  \n",
      "390587  1.000  0.000       0.0  \n",
      "8051    1.000  0.000       0.0  \n",
      "99746   1.000  0.000       0.0  \n",
      "380859  1.000  0.000       0.0  \n",
      "233215  1.000  0.000       0.0  \n",
      "28323   1.000  0.000       0.0  \n",
      "41166   1.000  0.000       0.0  \n",
      "38455   1.000  0.000       0.0  \n",
      "175939  1.000  0.000       0.0  \n",
      "68306   1.000  0.000       0.0  \n",
      "168025  1.000  0.000       0.0  \n"
     ]
    }
   ],
   "source": [
    "# Verificando quantas e quais linhas foram interpretadas como neutras\n",
    "print(df_amostra[df_amostra[\"compound\"] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletando as frases consideradas neutras\n",
    "neutral = df_amostra[df_amostra[\"compound\"] == 0]\n",
    "\n",
    "df_sentiment_vader = df_amostra.drop(neutral.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label       2942\n",
       "review      2942\n",
       "neg         2942\n",
       "neu         2942\n",
       "pos         2942\n",
       "compound    2942\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment_vader.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\018118631\\AppData\\Local\\Temp\\ipykernel_66324\\782170959.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if x[0] == 1 and x[5] > 0:\n",
      "C:\\Users\\018118631\\AppData\\Local\\Temp\\ipykernel_66324\\782170959.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  elif x[0] == 1 and x[5] < 0:\n",
      "C:\\Users\\018118631\\AppData\\Local\\Temp\\ipykernel_66324\\782170959.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  elif x[0] == -1 and x[5] > 0:\n",
      "C:\\Users\\018118631\\AppData\\Local\\Temp\\ipykernel_66324\\782170959.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  elif x[0] == -1 and x[5] < 0:\n"
     ]
    }
   ],
   "source": [
    "# Criando uma coluna para armazenar se foi um erro ou certo\n",
    "def conf_matrix(x):\n",
    "  if x[0] == 1 and x[5] > 0:\n",
    "    return 'TP'\n",
    "  elif x[0] == 1 and x[5] < 0:\n",
    "    return 'FN'\n",
    "  elif x[0] == -1 and x[5] > 0:\n",
    "    return 'FP'\n",
    "  elif x[0] == -1 and x[5] < 0:\n",
    "    return 'TN'\n",
    "  else:\n",
    "    return 0\n",
    "    \n",
    "df_sentiment_vader['Conf_Matrix'] = df_sentiment_vader.apply(lambda x: conf_matrix(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Conf_Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18589</th>\n",
       "      <td>1</td>\n",
       "      <td>I loved the packaging and texture of the produ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.8678</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                             review  neg    neu  \\\n",
       "18589      1  I loved the packaging and texture of the produ...  0.0  0.796   \n",
       "\n",
       "         pos  compound Conf_Matrix  \n",
       "18589  0.204    0.8678          TP  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment_vader.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 2374, 'FP': 335, 'TN': 148, 'FN': 85}\n",
      "Accuracy:  85.72 % \n",
      "Precision:  87.63 % \n",
      "Recall:  96.54 % \n",
      "F1_Score: 0.9187306501547989\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo algumas métricas interessantes\n",
    "conf_vals = df_sentiment_vader.Conf_Matrix.value_counts().to_dict()\n",
    "print(conf_vals)\n",
    "\n",
    "accuracy = (conf_vals['TP'] + conf_vals['TN']) / (conf_vals['TP'] + conf_vals['TN'] + conf_vals['FP'] + conf_vals['FN'])\n",
    "precision = conf_vals['TP'] / (conf_vals['TP'] + conf_vals['FP'])\n",
    "recall = conf_vals['TP'] / (conf_vals['TP'] + conf_vals['FN'])\n",
    "f1_score = 2*(precision*recall) / (precision + recall)\n",
    "print('Accuracy: ', round(100 * accuracy, 2),'%',\n",
    "      '\\nPrecision: ', round(100 * precision, 2),'%',\n",
    "      '\\nRecall: ', round(100 * recall, 2),'%',\n",
    "      '\\nF1_Score:', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos testar o RoBERTa no conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\018118631\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando o modelo de base\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o tokenizador e o modelo RoBERTa pré-treinado\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-processar o conjunto de dados\n",
    "def tokenization(text):\n",
    "    print(text)\n",
    "    return tokenizer(text['review'], return_tensors='pt', padding = True, truncation = True)\n",
    "\n",
    "\n",
    "df_amostra[\"tokenized_text\"] = df_amostra.apply(lambda row: tokenization, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19619</th>\n",
       "      <td>1</td>\n",
       "      <td>I love the moisturizer! It has such a great te...</td>\n",
       "      <td>&lt;function tokenization at 0x0000023397288860&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                             review  \\\n",
       "19619      1  I love the moisturizer! It has such a great te...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "19619  <function tokenization at 0x0000023397288860>  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amostra.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# Função para usar o modelo no conjunto de dados\n",
    "def roberta_polarity_scores(text):\n",
    "    encoded_text = tokenizer(text, return_tensors='pt',truncation=True, max_length=512)\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores_dict = {\n",
    "    'roberta_neg': scores[0],\n",
    "    'roberta_neu': scores[1],\n",
    "    'roberta_pos': scores[2]}\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m df_amostra[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m     result\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mroberta_polarity_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[42], line 6\u001b[0m, in \u001b[0;36mroberta_polarity_scores\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroberta_polarity_scores\u001b[39m(text):\n\u001b[0;32m      5\u001b[0m     encoded_text \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     scores \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      8\u001b[0m     scores \u001b[38;5;241m=\u001b[39m softmax(scores)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1198\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1210\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    834\u001b[0m )\n\u001b[1;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m         output_attentions,\n\u001b[0;32m    522\u001b[0m     )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:349\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    341\u001b[0m         hidden_states,\n\u001b[0;32m    342\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m         output_attentions,\n\u001b[0;32m    348\u001b[0m     )\n\u001b[1;32m--> 349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:300\u001b[0m, in \u001b[0;36mRobertaSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    298\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    299\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m--> 300\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2545\u001b[0m     )\n\u001b[1;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "for review in df_amostra['review']:\n",
    "    result.update(roberta_polarity_scores(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_amostra[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19619</th>\n",
       "      <td>1</td>\n",
       "      <td>I love the moisturizer! It has such a great te...</td>\n",
       "      <td>&lt;function tokenization at 0x0000023397288860&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                             review  \\\n",
       "19619      1  I love the moisturizer! It has such a great te...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "19619  <function tokenization at 0x0000023397288860>  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amostra.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        label                                             review  \\\n",
      "19619       1  I love the moisturizer! It has such a great te...   \n",
      "208964      1  So amazing it smells so good and doesnt dry my...   \n",
      "142324      1  I absolutely love this face mask. I use it eve...   \n",
      "7492        1  Gifted by the outset. This is the first time i...   \n",
      "569881      1  Love this stuff!  Makes my skin feel great wit...   \n",
      "\n",
      "                                       tokenized_text  roberta_neg  \\\n",
      "19619   <function tokenization at 0x0000023397288860>     0.001204   \n",
      "208964  <function tokenization at 0x0000023397288860>     0.002356   \n",
      "142324  <function tokenization at 0x0000023397288860>     0.003118   \n",
      "7492    <function tokenization at 0x0000023397288860>     0.005684   \n",
      "569881  <function tokenization at 0x0000023397288860>     0.001148   \n",
      "\n",
      "        roberta_neu  roberta_pos  compound  \n",
      "19619      0.007111     0.991685  0.990480  \n",
      "208964     0.007858     0.989787  0.987431  \n",
      "142324     0.018708     0.978174  0.975056  \n",
      "7492       0.017275     0.977041  0.971357  \n",
      "569881     0.007399     0.991453  0.990305  \n"
     ]
    }
   ],
   "source": [
    "def roberta_polarity_scores(text):\n",
    "    encoded_text = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores[0], scores[1], scores[2]\n",
    "\n",
    "# Aplicar a função roberta_polarity_scores a cada linha do DataFrame\n",
    "df_amostra['roberta_neg'], df_amostra['roberta_neu'], df_amostra['roberta_pos'] = zip(*df_amostra['review'].apply(roberta_polarity_scores))\n",
    "\n",
    "# Calcular o score compound\n",
    "df_amostra['compound'] = df_amostra['roberta_pos'] - df_amostra['roberta_neg']\n",
    "\n",
    "# Mostrar as primeiras linhas do DataFrame com as colunas adicionadas\n",
    "print(df_amostra.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vou ignorar frases consideradas neutras\n",
    "neutral = df_amostra[df_amostra[\"compound\"] == 0]\n",
    "\n",
    "df_sentiment_roberta = df_amostra.drop(neutral.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>roberta_neg</th>\n",
       "      <th>roberta_neu</th>\n",
       "      <th>roberta_pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19619</th>\n",
       "      <td>1</td>\n",
       "      <td>I love the moisturizer! It has such a great te...</td>\n",
       "      <td>&lt;function tokenization at 0x0000023397288860&gt;</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.007111</td>\n",
       "      <td>0.991685</td>\n",
       "      <td>0.99048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                             review  \\\n",
       "19619      1  I love the moisturizer! It has such a great te...   \n",
       "\n",
       "                                      tokenized_text  roberta_neg  \\\n",
       "19619  <function tokenization at 0x0000023397288860>     0.001204   \n",
       "\n",
       "       roberta_neu  roberta_pos  compound  \n",
       "19619     0.007111     0.991685   0.99048  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment_roberta.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3000.000000\n",
       "mean        0.679487\n",
       "std         0.556007\n",
       "min        -0.977956\n",
       "25%         0.741100\n",
       "50%         0.959727\n",
       "75%         0.983560\n",
       "max         0.991933\n",
       "Name: compound, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment_roberta[\"compound\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'review', 'tokenized_text', 'roberta_neg', 'roberta_neu',\n",
       "       'roberta_pos', 'compound', 'Conf_Matrix'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment_roberta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\018118631\\AppData\\Local\\Temp\\ipykernel_66324\\1852277079.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if x[0] == 1 and x[6] > 0:\n",
      "C:\\Users\\018118631\\AppData\\Local\\Temp\\ipykernel_66324\\1852277079.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  elif x[0] == 1 and x[6] < 0:\n",
      "C:\\Users\\018118631\\AppData\\Local\\Temp\\ipykernel_66324\\1852277079.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  elif x[0] == -1 and x[6] > 0:\n",
      "C:\\Users\\018118631\\AppData\\Local\\Temp\\ipykernel_66324\\1852277079.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  elif x[0] == -1 and x[6] < 0:\n"
     ]
    }
   ],
   "source": [
    "def conf_matrix(x):\n",
    "  if x[0] == 1 and x[6] > 0:\n",
    "    return 'TP'\n",
    "  elif x[0] == 1 and x[6] < 0:\n",
    "    return 'FN'\n",
    "  elif x[0] == -1 and x[6] > 0:\n",
    "    return 'FP'\n",
    "  elif x[0] == -1 and x[6] < 0:\n",
    "    return 'TN'\n",
    "  else:\n",
    "    return 0\n",
    "    \n",
    "df_sentiment_roberta['Conf_Matrix'] = df_sentiment_roberta.apply(lambda x: conf_matrix(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>roberta_neg</th>\n",
       "      <th>roberta_neu</th>\n",
       "      <th>roberta_pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Conf_Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19619</th>\n",
       "      <td>1</td>\n",
       "      <td>I love the moisturizer! It has such a great te...</td>\n",
       "      <td>&lt;function tokenization at 0x0000023397288860&gt;</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.007111</td>\n",
       "      <td>0.991685</td>\n",
       "      <td>0.990480</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208964</th>\n",
       "      <td>1</td>\n",
       "      <td>So amazing it smells so good and doesnt dry my...</td>\n",
       "      <td>&lt;function tokenization at 0x0000023397288860&gt;</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>0.989787</td>\n",
       "      <td>0.987431</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142324</th>\n",
       "      <td>1</td>\n",
       "      <td>I absolutely love this face mask. I use it eve...</td>\n",
       "      <td>&lt;function tokenization at 0x0000023397288860&gt;</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.018708</td>\n",
       "      <td>0.978174</td>\n",
       "      <td>0.975056</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7492</th>\n",
       "      <td>1</td>\n",
       "      <td>Gifted by the outset. This is the first time i...</td>\n",
       "      <td>&lt;function tokenization at 0x0000023397288860&gt;</td>\n",
       "      <td>0.005684</td>\n",
       "      <td>0.017275</td>\n",
       "      <td>0.977041</td>\n",
       "      <td>0.971357</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569881</th>\n",
       "      <td>1</td>\n",
       "      <td>Love this stuff!  Makes my skin feel great wit...</td>\n",
       "      <td>&lt;function tokenization at 0x0000023397288860&gt;</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>0.990305</td>\n",
       "      <td>TP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                             review  \\\n",
       "19619       1  I love the moisturizer! It has such a great te...   \n",
       "208964      1  So amazing it smells so good and doesnt dry my...   \n",
       "142324      1  I absolutely love this face mask. I use it eve...   \n",
       "7492        1  Gifted by the outset. This is the first time i...   \n",
       "569881      1  Love this stuff!  Makes my skin feel great wit...   \n",
       "\n",
       "                                       tokenized_text  roberta_neg  \\\n",
       "19619   <function tokenization at 0x0000023397288860>     0.001204   \n",
       "208964  <function tokenization at 0x0000023397288860>     0.002356   \n",
       "142324  <function tokenization at 0x0000023397288860>     0.003118   \n",
       "7492    <function tokenization at 0x0000023397288860>     0.005684   \n",
       "569881  <function tokenization at 0x0000023397288860>     0.001148   \n",
       "\n",
       "        roberta_neu  roberta_pos  compound Conf_Matrix  \n",
       "19619      0.007111     0.991685  0.990480          TP  \n",
       "208964     0.007858     0.989787  0.987431          TP  \n",
       "142324     0.018708     0.978174  0.975056          TP  \n",
       "7492       0.017275     0.977041  0.971357          TP  \n",
       "569881     0.007399     0.991453  0.990305          TP  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment_roberta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 2425, 'TN': 338, 'FP': 162, 'FN': 75}\n",
      "Accuracy:  92.1 % \n",
      "Precision:  93.74 % \n",
      "Recall:  97.0 % \n",
      "F1_Score: 0.9534106546097896\n"
     ]
    }
   ],
   "source": [
    "conf_vals = df_sentiment_roberta.Conf_Matrix.value_counts().to_dict()\n",
    "print(conf_vals)\n",
    "\n",
    "accuracy = (conf_vals['TP'] + conf_vals['TN']) / (conf_vals['TP'] + conf_vals['TN'] + conf_vals['FP'] + conf_vals['FN'])\n",
    "precision = conf_vals['TP'] / (conf_vals['TP'] + conf_vals['FP'])\n",
    "recall = conf_vals['TP'] / (conf_vals['TP'] + conf_vals['FN'])\n",
    "f1_score = 2*(precision*recall) / (precision + recall)\n",
    "print('Accuracy: ', round(100 * accuracy, 2),'%',\n",
    "      '\\nPrecision: ', round(100 * precision, 2),'%',\n",
    "      '\\nRecall: ', round(100 * recall, 2),'%',\n",
    "      '\\nF1_Score:', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histogramas de Erros e Acertos\n",
    "plt.hist(df_sentiment_roberta['Conf_Matrix'], alpha=0.5, label='RoBERTa', color=\"Red\", align='left')\n",
    "plt.hist(df_sentiment_vader['Conf_Matrix'], alpha=0.5, label='VADER', color=\"Blue\", align='right')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"RoBERTa vs. VADER\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o dataset em questão, o RoBERTa performa melhor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
